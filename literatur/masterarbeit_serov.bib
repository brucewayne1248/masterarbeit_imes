% Encoding: UTF-8

@Article{Li2017,
  author         = {Yuxi Li},
  title          = {Deep Reinforcement Learning: An Overview},
  year           = {2017},
  month          = jan,
  abstract       = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  eprint         = {1701.07274},
  oai2identifier = {1701.07274},
  owner          = {andi},
  timestamp      = {2018.03.26},
}

@InProceedings{Ng1999,
  Title                    = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  Author                   = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  Booktitle                = {Proceedings of the Sixteenth International Conference on Machine Learning},
  Year                     = {1999},

  Address                  = {San Francisco, CA, USA},
  Pages                    = {278--287},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Series                   = {ICML '99},

  Acmid                    = {657613},
  ISBN                     = {1-55860-612-2},
  Numpages                 = {10},
  Url                      = {http://dl.acm.org/citation.cfm?id=645528.657613}
}

@Article{Peters2008,
  author    = {Jan Peters and Stefan Schaal},
  title     = {Reinforcement learning of motor skills with policy gradients},
  journal   = {Neural Networks},
  year      = {2008},
  volume    = {21},
  number    = {4},
  pages     = {682 - 697},
  issn      = {0893-6080},
  note      = {Robotics and Neuroscience},
  doi       = {https://doi.org/10.1016/j.neunet.2008.02.003},
  keywords  = {Reinforcement learning, Policy gradient methods, Natural gradients, Natural Actor-Critic, Motor skills, Motor primitives},
  owner     = {andi},
  timestamp = {2018.03.26},
  url       = {http://www.sciencedirect.com/science/article/pii/S0893608008000701},
}

@InProceedings{Kakade2002,
  author    = {Kakade, Sham M},
  title     = {A natural policy gradient},
  booktitle = {Advances in neural information processing systems},
  year      = {2002},
  pages     = {1531--1538},
  owner     = {andi},
  timestamp = {2018.03.27},
}

@Article{Kober2013,
  author    = {Jens Kober and J. Andrew Bagnell and Jan Peters},
  title     = {Reinforcement learning in robotics: A survey},
  journal   = {The International Journal of Robotics Research},
  year      = {2013},
  volume    = {32},
  number    = {11},
  pages     = {1238-1274},
  abstract  = { Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research. },
  doi       = {10.1177/0278364913495721},
  eprint    = {https://doi.org/10.1177/0278364913495721},
  owner     = {andi},
  timestamp = {2018.03.26},
  url       = {https://doi.org/10.1177/0278364913495721},
}

@Article{Mnih2016,
  author       = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  title        = {Asynchronous Methods for Deep Reinforcement Learning},
  abstract     = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  date         = {2016-02-04},
  eprint       = {1602.01783v2},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {online:http\://arxiv.org/pdf/1602.01783v2:PDF},
  journaltitle = {ICML 2016},
  keywords     = {cs.LG},
  owner        = {andi},
  timestamp    = {2018.03.28},
}

@Article{Duan2016,
  author      = {Yan Duan and Xi Chen and Rein Houthooft and John Schulman and Pieter Abbeel},
  title       = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  abstract    = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
  date        = {2016-04-22},
  eprint      = {1604.06778v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1604.06778v3:PDF},
  keywords    = {cs.LG, cs.AI, cs.RO},
  owner       = {andi},
  timestamp   = {2018.04.02},
}

@InProceedings{Nguyen2015,
  author    = {T. D. Nguyen and J. Burgner-Kahrs},
  title     = {A tendon-driven continuum robot with extensible sections},
  booktitle = {Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)},
  year      = {2015},
  pages     = {2130--2135},
  month     = sep,
  doi       = {10.1109/IROS.2015.7353661},
  keywords  = {design engineering, discs (structures), manipulators, permanent magnets, 3D space, alternating pole orientation, equidistant disk spacing, extensible sections, fixed section lengths, follow-the-leader fashion, linear translation, magnetic repulsion forces, permanent magnets, rigid link serial manipulators, spacer disks, telescoping backbone, tendon actuation, tendon-driven continuum robot, tendon-driven robot designs, tortuous paths, Electron tubes, Manipulators, Neodymium, Pneumatic systems, Prototypes, Tendons},
  owner     = {andi},
  timestamp = {2018.04.03},
}

@Article{WebsterIII2010,
  author    = {Webster III, Robert J and Jones, Bryan A},
  title     = {Design and kinematic modeling of constant curvature continuum robots: A review},
  journal   = {The International Journal of Robotics Research},
  year      = {2010},
  volume    = {29},
  number    = {13},
  pages     = {1661--1683},
  owner     = {andi},
  publisher = {SAGE Publications Sage UK: London, England},
  timestamp = {2018.04.04},
}

@Article{Jones2006,
  author    = {B. A. Jones and I. D. Walker},
  title     = {Kinematics for multisection continuum robots},
  journal   = IEEE_J_RO,
  year      = {2006},
  volume    = {22},
  number    = {1},
  pages     = {43--55},
  month     = feb,
  issn      = {1552-3098},
  doi       = {10.1109/TRO.2005.861458},
  keywords  = {manipulator kinematics, shape control, actuator inputs, continuous backbone robots, multisection continuum robot kinematics, physical manipulator constraints, pneumatic pressures, robot shape coordinates, shape control, spatial multisection continuum manipulators, tendon lengths, workspace Cartesian coordinates, Hardware, Legged locomotion, Manipulators, Medical robotics, Pneumatic actuators, Robot kinematics, Robot sensing systems, Shape control, Spine, Tendons, Biologically inspired robots, continuum robot, kinematics, tentacle, trunk},
  owner     = {andi},
  timestamp = {2018.04.04},
}

@Article{Jones2006a,
  author    = {B. A. Jones and I. D. Walker},
  title     = {Practical Kinematics for Real-Time Implementation of Continuum Robots},
  journal   = IEEE_J_RO,
  year      = {2006},
  volume    = {22},
  number    = {6},
  pages     = {1087--1099},
  month     = dec,
  issn      = {1552-3098},
  doi       = {10.1109/TRO.2006.886268},
  keywords  = {manipulator kinematics, continuum manipulators, finite actuation mechanisms, multisection tendon-actuated continuum robots, practical kinematics, Actuators, Arm, Hardware, Joints, Kinematics, Legged locomotion, Manipulators, Orbital robotics, Robot control, Shape, Biologically inspired robots, continuum robot, kinematics, tentacle, trunk},
  owner     = {andi},
  timestamp = {2018.04.04},
}

@Article{Camarillo2009,
  author        = {D. B. Camarillo and C. R. Carlson and J. K. Salisbury},
  title         = {Configuration Tracking for Continuum Manipulators With Coupled Tendon Drive},
  journal       = IEEE_J_RO,
  year          = {2009},
  volume        = {25},
  number        = {4},
  pages         = {798--808},
  month         = aug,
  issn          = {1552-3098},
  __markedentry = {[andi:]},
  doi           = {10.1109/TRO.2009.2022426},
  keywords      = {biological tissues, catheters, flexible manipulators, manipulator kinematics, medical robotics, cardiac catheter, configuration tracking, continuum manipulator, coupled tendon drive, decoupled inverse kinematics, flexible device, forward kinematics, geometrical coupling, linear beam configuration, mechanical coupling, medical procedure, robotic control, shape configuration, tendon displacement, Cable drive, continuum robot, flexible arm, medical robot},
  owner         = {andi},
  timestamp     = {2018.04.04},
}

@InCollection{Mataric1994,
  author        = {Mataric, Maja J},
  title         = {Reward functions for accelerated learning},
  booktitle     = {Machine Learning Proceedings 1994},
  publisher     = {Elsevier},
  year          = {1994},
  pages         = {181--189},
  __markedentry = {[andi:6]},
  owner         = {andi},
  timestamp     = {2018.04.23},
}

@Comment{jabref-meta: databaseType:bibtex;}
